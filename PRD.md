
## 1 向量数学基础与核心原理

---

### 1.1 文本向量化（Embeddings）

在职位分类中，系统必须回答：  
“**两个描述同一种职业，但写法完全不同的文本，如何判断它们语义相似？**”

传统关键词匹配（如 TF-IDF、词袋模型）无法处理：
- 语义近义词（如 “网络安全工程师” ≈ “信息安全专员”）
- 上下文关联（如职责 A+B 组合意义）
- 行业术语抽象概念（如 “攻击面分析”）

因此系统选用 `quentinz/bge-large-zh-v1.5`，该模型将任意中文文本转换成 **1024 维语义向量**：

```
v = E(T),  v ∈ ℝ¹⁰²⁴
```

这里每一维不代表可解释的“词”，而代表 Transformer 自注意力层捕捉的语义特征。  
Transformer 的能力使模型不仅提取词，还能“理解”：
- 上下文依赖
- 词语之间的相互影响
- 职责内部的逻辑关系

**为什么选 1024 维？**  
维度过低 → 表达能力不足  
维度过高 → 检索变慢且噪声增多  
1024 被实验验证为中文语义任务（尤其是长文本）中的最佳折中。

---

### 1.2 L2 归一化（L2 Normalization）

向量化后的文本仍存在一个关键问题：

**不同长度的文本模长不同，会影响相似度计算。**

例如：
- 一段详细的 JD（长向量）
- 一段简洁的职业定义（短向量）

在原始空间中，长文本向量通常模长更大，会在点积运算中占据不公平优势。

为消除这种偏差，对每个向量执行 L2 归一化：

```
||v||₂ = sqrt(Σ vᵢ²)
v_norm = v / ||v||₂
```

所有向量被投影到单位球面，使系统“只关注方向”。  
方向 = 语义。  
模长 = 文本长度 → **应当被消除**。

工程意义：
1. 不会因为 JD 写得长而获得“虚高”相似度  
2. 后续相似度计算更快（余弦相似度简化为点积）  
3. ChromaDB 内部优化中，归一化向量能更高效地计算

---

### 1.3 余弦相似度的选择

在向量空间中，最常见的两个衡量方式是欧氏距离与余弦相似度：

- 欧氏距离：关注“幅度 + 方向”，不适合文本  
- 余弦相似度：只关注方向（语义）

数学定义：

```
Similarity(A, B) = cosθ = Σ(Aᵢ * Bᵢ)
```

范围 `[-1, 1]`，越接近 1 越相似。

**为什么必须用余弦相似度而不是欧氏距离？**

因为在语义空间中：
- “两段文本意思是否一致”对应“方向是否一致”
- “文本写多少字”不应影响相似度（L2 已解决）

因此余弦相似度非常适合招聘语义匹配任务。

---

### 1.4 ChromaDB 的余弦距离定义

ChromaDB 的“cosine”模式本质上不是直接返回 cosθ，而是返回距离：

```
distance = 1 - cosθ
score = 1 - distance = cosθ
```

这使得：
- distance 越小 → 越相似  
- score 越大 → 语义越接近  

**为什么 ChromaDB 要返回 distance 而不是 score？**  
因为在向量数据库的工程设计中，“最小化距离”是最符合传统检索算法（如 KNN、HNSW）的做法。

系统在业务层进行反向转换，是工程上最自然的做法。

---

## 2 系统技术选型与理由


### 2.1 Embedding 模型：`quentinz/bge-large-zh-v1.5`

选择理由：

1. **中文任务第一梯队**  
   与通用英文模型相比，该模型在中文语法结构、行业词汇嵌入方面更精细。
2. **C-MTEB 榜单实证验证**  
   在中文文本相似度/检索挑战中排名靠前。
3. **兼顾速度与效果**  
   1024 维向量可同时保证：
   - 语义表示足够丰富  
   - ChromaDB 检索不会过慢

最终效果是：  
**职业描述和 JD 的语义空间非常接近，可稳定用于分类。**

---

### 2.2 向量数据库：ChromaDB

选用 ChromaDB 而非 Milvus、Weaviate 的原因包括：

1. **部署简单**  
   本地即可使用，无需维护复杂分布式集群。
2. **支持 cosine 空间**  
   完美契合本系统的数学模型。
3. **轻量且“just works”**  
   对中小规模职业库（几千～几万向量）检索延迟极低。

工程角度：  
ChromaDB 是本地化 AI 应用的最佳平衡点——轻量、高效、稳定。

---

### 2.3 JD 结构化 LLM：Ollama + `qwen2:7b-instruct`

要点：

1. **招聘 JD 通常包含企业敏感信息**  
   不能发到第三方云 API，因此必须本地化。
2. **模型参数够小，能在本地跑**  
   qwen2:7b-instruct 只需要中等显卡即可。
3. **结构化输出稳定**  
   对 JSON 结构提取非常可靠，适合做 “清洗 → 标准化” 步骤。

---

### 2.4 职业库增强 LLM：`deepseek-v3`

职业库本身不是敏感数据，因此可以交由强大的 API 模型完成增强：
- 岗位概述
- 典型职责
- 关键词
- 岗位别称

增强后的内容为向量生成提供更完整的语义上下文，能极大提高匹配准确度。

这是“离线一次性任务”，无需担心成本或隐私。

---

## 3 系统数据流与处理流程


### 3.1 阶段一：职业库增强与向量化

命令：`cmd/jobimport`, `cmd/classifier`

#### 3.1.1 步骤 1：使用 deepseek-v3 增强职业描述

职业大典原文本往往内容简略，不足以填满语义向量空间。  
通过 deepseek-v3 进行扩写，使模型生成的 embedding 更稳定。

示例：

```json
{
  "大类": "生产制造",
  "细类（职业）": "数控车床操作工",
  "LLM岗位概述": "数控车床操作工是精密制造业的关键岗位...",
  "LLM典型职责": ["解读技术图纸...", "设置和操作数控车床..."],
  "LLM关联关键词": ["CNC", "精密加工", "G代码"],
  "LLM典型岗位": ["CNC车床操作员", "数控技工"]
}
```

数学意义：  
增强后的文本覆盖更多语义维度 → embedding 更能代表职业。

---

#### 3.1.2 步骤 2：Embedding + L2归一化 + 存入 ChromaDB

长文本 → BGE 模型 → 得到 1024 维向量 → 归一化后：

```json
{
  "id": "uuid-generated-for-job",
  "embedding": [0.021, -0.034, ..., 0.015],
  "metadata": { "细类（职业）": "数控车床操作工" }
}
```

为什么要归一化？  
因为后续 ChromaDB 中的“cosine”搜索本质上依赖点积，而点积在单位向量空间代表真实余弦相似度。

---

### 3.2 阶段二：招聘岗位结构化

命令：`cmd/importer`

JD 原文通常带噪声、无格式、含口语化内容。

示例输入：

```
岗位职责：1. 负责公司网络安全体系... 熟悉TCP/IP协议...
```

结构化输出：

```json
{
  "job_name": "网络安全工程师",
  "summary": "负责企业整体网络安全体系的建设与运维...",
  "responsibilities": ["规划与建设网络安全体系","监控安全事件"],
  "skills": ["TCP/IP", "防火墙", "WAF"]
}
```

此结构化数据用于下一步 embedding，使各 JD 在统一结构下被转换为向量，提高匹配稳定性。

---

### 3.3 阶段三：向量查询与匹配

命令：`cmd/query`

#### 3.3.1 步骤 1：生成查询向量

结构化字段拼接后 → Embedding → L2 归一化。  
数学处理与职业库完全一致，使得语义空间统一。

---

#### 3.3.2 步骤 2：ChromaDB 相似度搜索

```json
{
  "id": "uuid-of-matched-job",
  "distance": 0.18,
  "metadata": { "细类（职业）": "网络与信息安全管理员" }
}
```

ChromaDB 内部执行的是： 

**基于 HNSW 图的快速近似 KNN 搜索 + cosine 距离**

---

#### 3.3.3 步骤 3：相似度分数计算

```
score = 1 - distance = 0.82
```

score 越高 → 越符合该 JD 的语义描述。

---

#### 3.3.4 最终输出示例

```json
{
  "query_text": "岗位名称: 网络安全工程师...",
  "results": [
    {
      "id": "uuid-of-matched-job",
      "score": 0.82,
      "metadata": { "细类（职业）": "网络与信息安全管理员" }
    }
  ]
}
```